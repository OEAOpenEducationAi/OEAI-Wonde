{
  "nbformat": 4,
  "nbformat_minor": 2,
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "%run oeai_py"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "# CHANGE VALUES FOR YOUR KEY VAULT\r\n",
        "keyvault = \"kv-oea-oeai\"  \r\n",
        "keyvault_linked_service = \"LS_KeyVault\"  \r\n",
        "\r\n",
        "# Synapse OEA environment paths\r\n",
        "silver_path = oeai.get_secret(spark, \"wonde-silver\", keyvault_linked_service, keyvault)\r\n",
        "gold_path = oeai.get_secret(spark, \"gold-path\", keyvault_linked_service, keyvault)\r\n",
        "storage_account_name = oeai.get_secret(spark, \"storage-account\", keyvault_linked_service, keyvault)\r\n",
        "storage_account_access_key = oeai.get_secret(spark, \"storage-accesskey\", keyvault_linked_service, keyvault)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "def process_delta_tables_to_parquet(spark, storage_account_name, storage_account_access_key, silver_path, gold_path):\r\n",
        "    \"\"\"\r\n",
        "    Sets up configuration for Azure storage access, lists subdirectories in the silver path, and processes\r\n",
        "    each Delta Lake table by converting and saving it in Parquet format in the gold path.\r\n",
        "\r\n",
        "    Args:\r\n",
        "        spark (SparkSession): Active Spark session.\r\n",
        "        storage_account_name (str): Azure storage account name.\r\n",
        "        storage_account_access_key (str): Access key for the Azure storage account.\r\n",
        "        silver_path (str): Path to the silver layer directory (source Delta tables).\r\n",
        "        gold_path (str): Path to the gold layer directory (destination for Parquet files).\r\n",
        "\r\n",
        "    This function will process each Delta Lake table found in the silver layer, partition the data by \r\n",
        "    'organisationkey', and write it to the gold layer as Parquet files.\r\n",
        "    \"\"\"\r\n",
        "    # Set up the configuration for accessing the storage account\r\n",
        "    spark.conf.set(f\"fs.azure.account.key.{storage_account_name}.dfs.core.windows.net\", storage_account_access_key)\r\n",
        "\r\n",
        "    sc = spark.sparkContext\r\n",
        "    hadoop_conf = sc._jsc.hadoopConfiguration()\r\n",
        "    hadoop_conf.set(\"fs.azure\", \"org.apache.hadoop.fs.azure.NativeAzureFileSystem\")\r\n",
        "    hadoop_conf.set(\"fs.azure.account.key.\" + storage_account_name + \".blob.core.windows.net\", storage_account_access_key)\r\n",
        "\r\n",
        "    # URI for the parent directory\r\n",
        "    parent_dir_uri = sc._gateway.jvm.java.net.URI(silver_path)\r\n",
        "\r\n",
        "    # Hadoop Path of the parent directory\r\n",
        "    Path = sc._gateway.jvm.org.apache.hadoop.fs.Path\r\n",
        "    FileSystem = sc._gateway.jvm.org.apache.hadoop.fs.FileSystem\r\n",
        "\r\n",
        "    # Get the FileSystem for the given URI and configuration\r\n",
        "    fs = FileSystem.get(parent_dir_uri, hadoop_conf)\r\n",
        "\r\n",
        "    # List the subdirectories at the given URI\r\n",
        "    status = fs.listStatus(Path(silver_path))\r\n",
        "    delta_table_paths = [file.getPath().toString() for file in status if file.isDirectory()]\r\n",
        "\r\n",
        "    for table_path in delta_table_paths:\r\n",
        "        try:\r\n",
        "            df = spark.read.format(\"delta\").load(table_path)\r\n",
        "            table_name = os.path.basename(urlparse(table_path).path)\r\n",
        "            parquet_output_folder_path = os.path.join(gold_path, table_name)\r\n",
        "            \r\n",
        "            df = df.withColumn(\"partitionkey\", col(\"organisationkey\"))\r\n",
        "            df.write.partitionBy(\"partitionkey\").mode(\"overwrite\").format(\"parquet\").save(parquet_output_folder_path)\r\n",
        "            \r\n",
        "        except AnalysisException as e:\r\n",
        "            print(f\"Error reading Delta table at {table_path}: \", e)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "# Initialize Spark session\r\n",
        "spark = SparkSession.builder.appName(\"oeaiSpark\").getOrCreate()\r\n",
        "process_delta_tables_to_parquet(spark, storage_account_name, storage_account_access_key, silver_path, gold_path)"
      ]
    }
  ],
  "metadata": {
    "save_output": true,
    "kernelspec": {
      "name": "synapse_pyspark",
      "display_name": "Synapse PySpark"
    },
    "language_info": {
      "name": "python"
    }
  }
}