{
  "nbformat": 4,
  "nbformat_minor": 2,
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "%run oeai_py"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "# CHANGE VALUES FOR YOUR KEY VAULT\r\n",
        "keyvault = \"kv-oea-oeai\"  \r\n",
        "keyvault_linked_service = \"LS_KeyVault\"  \r\n",
        "\r\n",
        "# Synapse OEA environment paths\r\n",
        "bronze_path = oeai.get_secret(spark, \"wonde-bronze\", keyvault_linked_service, keyvault)\r\n",
        "school_ids_secret = oeai.get_secret(spark, \"school-ids\", keyvault_linked_service, keyvault)\r\n",
        "school_ids = school_ids_secret.split(\",\")\r\n",
        "\r\n",
        "# Set up date parameters\r\n",
        "today = datetime.today()\r\n",
        "last_year = today - timedelta(days=365)\r\n",
        "DateFrom = last_year.strftime('%Y-%m-%d')\r\n",
        "DateTo = today.strftime('%Y-%m-%d')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "# initialise the audit log\r\n",
        "audit_log = oeai.load_audit_log(spark, bronze_path + \"audit_log.json\")\r\n",
        "audit_logs = []\r\n",
        "error_log_path = bronze_path + \"error_log.txt\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "def get_school_data(token: str, school_id: str, endpoint: str, query: str, pagination_type: str = \"cursor\") -> dict:\r\n",
        "    \"\"\"\r\n",
        "    Fetches data for a specific school using the Wonde API.\r\n",
        "\r\n",
        "    Args:\r\n",
        "        token (str): The authentication token for the Wonde API.\r\n",
        "        school_id (str): The unique identifier of the school.\r\n",
        "        endpoint (str): The specific endpoint of the API to query.\r\n",
        "        query (str): Additional query parameters for the API call.\r\n",
        "        pagination_type (str, optional): Type of pagination to use. Defaults to 'cursor'.\r\n",
        "            Supported values: 'cursor', 'offset'.\r\n",
        "\r\n",
        "    Returns:\r\n",
        "        dict: A collection of data for the specified school.\r\n",
        "    \"\"\"\r\n",
        "\r\n",
        "    # Format the endpoint URL\r\n",
        "    endpoint = endpoint.rstrip('/') + '/'\r\n",
        "\r\n",
        "    # Construct the base URL\r\n",
        "    base_url = f\"https://api.wonde.com/v1.0/schools/{school_id}{endpoint}\"\r\n",
        "    \r\n",
        "    # Format the query string\r\n",
        "    query = f\"?{query.lstrip('&?')}\"\r\n",
        "\r\n",
        "    url = base_url + query\r\n",
        "    headers = {\r\n",
        "        \"Authorization\": f\"Bearer {token}\"\r\n",
        "    }\r\n",
        "\r\n",
        "    all_data = []\r\n",
        "    next_url = url\r\n",
        "    page = 1\r\n",
        "    per_page_limit = 50  # Limit for number of items per page\r\n",
        "\r\n",
        "    while next_url:\r\n",
        "        # Adjust URL based on pagination type\r\n",
        "        if pagination_type == \"offset\":\r\n",
        "            paginated_url = f\"{url}&page={page}\"\r\n",
        "        else:\r\n",
        "            paginated_url = next_url\r\n",
        "\r\n",
        "        response = requests.get(paginated_url, headers=headers)\r\n",
        "\r\n",
        "        # Handle unsuccessful requests\r\n",
        "        if response.status_code != 200:\r\n",
        "            error_message = f\"Error fetching data from {paginated_url}: {response.status_code} {traceback.format_exc()}\"\r\n",
        "            oeai.log_error(spark, error_message, error_log_path)\r\n",
        "            break\r\n",
        "\r\n",
        "        response_data = response.json()\r\n",
        "\r\n",
        "        # Extract data from response\r\n",
        "        data_from_response = response_data.get(\"data\", [])\r\n",
        "        \r\n",
        "        # Append data based on its type (list or dict)\r\n",
        "        if isinstance(data_from_response, dict):\r\n",
        "            all_data.append(data_from_response)\r\n",
        "        else:\r\n",
        "            all_data.extend(data_from_response)\r\n",
        "\r\n",
        "        # Handle pagination logic\r\n",
        "        if pagination_type == \"cursor\":\r\n",
        "            next_url = response_data.get(\"meta\", {}).get(\"pagination\", {}).get(\"next\")\r\n",
        "        elif pagination_type == \"offset\":\r\n",
        "            if len(data_from_response) < per_page_limit:\r\n",
        "                break  # End loop if fewer items than per_page_limit\r\n",
        "            page += 1\r\n",
        "            next_url = f\"{url}&page={page}\"\r\n",
        "\r\n",
        "    return all_data\r\n",
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "def load_bronze(spark, endpoint: str, subkey: str, school_id: str, token: str, pagination_type, limit=None, query=None, use_date_chunk=False, has_students_array:bool=False, audit_log_file=\"audit_log.json\", override_date=None):\r\n",
        "    \"\"\"\r\n",
        "    Loads data from an API into a Bronze layer, handling pagination, date chunking, and audit logging.\r\n",
        "\r\n",
        "    Args:\r\n",
        "        spark (SparkSession): Active SparkSession for DataFrame operations.\r\n",
        "        endpoint (str): API endpoint to retrieve data from.\r\n",
        "        subkey (str): Subkey for identifying the specific data.\r\n",
        "        school_id (str): Unique identifier for the school.\r\n",
        "        token (str): Authentication token for API access.\r\n",
        "        pagination_type (str): Type of pagination used by the API ('cursor' or 'offset').\r\n",
        "        limit (int, optional): Limit for the number of records to retrieve. Defaults to None.\r\n",
        "        query (str, optional): Additional query parameters for the API call. Defaults to None.\r\n",
        "        use_date_chunk (bool, optional): Flag to indicate if date chunking is to be used. Defaults to False.\r\n",
        "        has_students_array (bool, optional): Flag to indicate if the data contains a students array. Defaults to False.\r\n",
        "        audit_log_file (str, optional): Filename for the audit log. Defaults to \"audit_log.json\".\r\n",
        "        override_date (str, optional): Date string in 'YYYY-MM-DD HH:MM:SS' format to override the last updated logic.\r\n",
        "        \r\n",
        "    Returns:\r\n",
        "        DataFrame: A PySpark DataFrame with the loaded data.\r\n",
        "    \"\"\"\r\n",
        "    global audit_log\r\n",
        "    df = pd.DataFrame()\r\n",
        "    data_list = [] \r\n",
        "    full_data_list = []\r\n",
        "    last_updated_str = None\r\n",
        "\r\n",
        "    # Calculate the duration of the API call\r\n",
        "    start_time = datetime.now()\r\n",
        "    now = datetime.now()\r\n",
        "    \r\n",
        "    if override_date:\r\n",
        "        last_updated_time = datetime.strptime(override_date, \"%Y-%m-%d %H:%M:%S\")\r\n",
        "    else:\r\n",
        "        last_updated_str = oeai.safe_get_or_create(LastUpdated, \"2018-09-01 00:00:00\", school_id, subkey)\r\n",
        "        if last_updated_str is None:\r\n",
        "            last_updated_time = now - timedelta(weeks=2)\r\n",
        "        elif isinstance(last_updated_str, str):\r\n",
        "            last_updated_time = datetime.strptime(last_updated_str, \"%Y-%m-%d %H:%M:%S\")\r\n",
        "        elif isinstance(last_updated_str, datetime):\r\n",
        "            last_updated_time = last_updated_str\r\n",
        "        else:\r\n",
        "            last_updated_time = now - timedelta(weeks=2)\r\n",
        "\r\n",
        "    # If last_updated_time is more than two weeks ago, chunk the requests\r\n",
        "    if use_date_chunk and (now - last_updated_time).days > 14:\r\n",
        "        for start_date, end_date in oeai.generate_date_chunks(last_updated_time, now, chunk_size=timedelta(weeks=2)):\r\n",
        "            chunk_query = oeai.update_query_with_chunks(query, start_date, end_date)\r\n",
        "            r = get_school_data(token, school_id, endpoint, chunk_query, pagination_type)\r\n",
        "\r\n",
        "            # Check if the response is not None and not empty before processing\r\n",
        "            if r:\r\n",
        "                if isinstance(r, dict) and 'data' in r:\r\n",
        "                    data_list.append(r['data'])\r\n",
        "                elif isinstance(r, list):\r\n",
        "                    data_list.extend(r)\r\n",
        "            else:\r\n",
        "                error_message = f\"Empty response, not adding to data_list: {traceback.format_exc()}\"\r\n",
        "                oeai.log_error(spark, error_message, error_log_path)\r\n",
        "    else:\r\n",
        "        if not override_date and last_updated_str is not None:\r\n",
        "            query += \"&updated_after=\" + last_updated_str\r\n",
        "        \r\n",
        "        r = get_school_data(token, school_id, endpoint, query, pagination_type)\r\n",
        "\r\n",
        "        # Ensure the data is always a list\r\n",
        "        if isinstance(r, dict) and 'data' in r:\r\n",
        "            data_list = [r['data']]\r\n",
        "        elif isinstance(r, list):\r\n",
        "            data_list = r\r\n",
        "\r\n",
        "    # Construct the directory path\r\n",
        "    school_folder = os.path.join(bronze_path, school_id)\r\n",
        "\r\n",
        "    # Check and create directory if it doesn't exist\r\n",
        "    if not os.path.exists(school_folder):\r\n",
        "        os.makedirs(school_folder)\r\n",
        "\r\n",
        "    LastUpdated[school_id][subkey] = datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\r\n",
        "    json_str = json.dumps(LastUpdated)\r\n",
        "    last_updated_df = spark.createDataFrame([LastUpdated])\r\n",
        "    last_updated_df.repartition(1).write.mode(\"overwrite\").json(bronze_path + 'last_run')\r\n",
        "\r\n",
        "    if not data_list:\r\n",
        "        oeai.save_empty_json(spark, school_folder + \"/\" + subkey + \".json\")\r\n",
        "    else:\r\n",
        "        try:\r\n",
        "            # Flatten each item in data_list\r\n",
        "            if has_students_array:\r\n",
        "                flattened_data_list = oeai.flatten_nested_json(json.dumps(data_list))\r\n",
        "            else:\r\n",
        "                flattened_data_list = [oeai.flatten_json(item) for item in data_list]\r\n",
        "\r\n",
        "            # Convert the list of dictionaries to a Pandas DataFrame\r\n",
        "            pandas_df = pd.DataFrame(flattened_data_list)\r\n",
        "\r\n",
        "            # Convert the Pandas DataFrame to a PySpark DataFrame\r\n",
        "            r_df = spark.createDataFrame(pandas_df)\r\n",
        "\r\n",
        "            # Add school_id and unique_key to the DataFrame\r\n",
        "            r_df = r_df.withColumn(\"school_id\", lit(school_id))\r\n",
        "            if \"student_data_id\" in r_df.columns:\r\n",
        "                r_df = r_df.withColumn(\"unique_key\", concat(lit(school_id),r_df[\"student_data_id\"].cast(\"string\"), r_df[\"id\"].cast(\"string\")))\r\n",
        "            else:\r\n",
        "                r_df = r_df.withColumn(\"unique_key\", concat(lit(school_id), r_df[\"id\"].cast(\"string\")))\r\n",
        "\r\n",
        "            # Save the DataFrame to a JSON file\r\n",
        "            r_df.write.mode(\"overwrite\").json(school_folder + \"/\" + subkey + \".json\")\r\n",
        "      \r\n",
        "        # if the key doesn't exist, skip it    \r\n",
        "        except Exception as e:\r\n",
        "            error_message = f\"Error: {traceback.format_exc()}\"\r\n",
        "            oeai.log_error(spark, error_message, error_log_path)\r\n",
        "            pass\r\n",
        "\r\n",
        "    # Update the audit log\r\n",
        "    end_time = datetime.now()\r\n",
        "    duration = (end_time - start_time).total_seconds()\r\n",
        "    duration_str = str(duration)\r\n",
        "    audit_data = {\r\n",
        "        \"school_id\": school_id,\r\n",
        "        \"endpoint\": endpoint,\r\n",
        "        \"query\": query,\r\n",
        "        \"start_time\": start_time.strftime('%Y-%m-%d %H:%M:%S'),\r\n",
        "        \"end_time\": end_time.strftime('%Y-%m-%d %H:%M:%S'),\r\n",
        "        \"duration\": duration_str,\r\n",
        "        \"records_returned\": str(len(data_list)),\r\n",
        "    }\r\n",
        "    audit_log.append(audit_data)\r\n",
        "\r\n",
        "    return df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 107,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "'''\r\n",
        "  BRONZE PROCESS\r\n",
        "'''\r\n",
        "# introduce a limit for testing or leave as None for Live\r\n",
        "Limit = None\r\n",
        "\r\n",
        "# Populate the schools list with the Wonde school_id and secret\r\n",
        "schools_list = []\r\n",
        "for school_id in school_ids:\r\n",
        "    secret_name = f\"wonde-{school_id}\"\r\n",
        "    try:\r\n",
        "        token = oeai.get_secret(spark, secret_name, keyvault_linked_service, keyvault)\r\n",
        "        schools_list.append({\"school_id\": school_id, \"token\": token})\r\n",
        "    except Exception as e:\r\n",
        "        error_message = f\"Error: {traceback.format_exc()}\"\r\n",
        "        oeai.log_error(spark, error_message, error_log_path)\r\n",
        "\r\n",
        "# Reset LastUpdated at the beginning of your read or function block\r\n",
        "LastUpdated = {}\r\n",
        "\r\n",
        "# Read the JSON file using PySpark\r\n",
        "df = spark.read.json(bronze_path + 'last_run')\r\n",
        "rows = df.collect()\r\n",
        "if rows:\r\n",
        "    LastUpdated = rows[0].asDict()\r\n",
        "else:\r\n",
        "    # Handle the case where the JSON file might be empty or not read correctly\r\n",
        "    LastUpdated = {}\r\n",
        "\r\n",
        "# Convert 'LastUpdated' Row objects to dictionaries\r\n",
        "for key, value in LastUpdated.items():\r\n",
        "    if isinstance(value, Row):\r\n",
        "        LastUpdated[key] = oeai.row_to_dict(value)\r\n",
        "\r\n",
        "for school in schools_list:\r\n",
        "    school_id = school[\"school_id\"]\r\n",
        "    token = school[\"token\"]  \r\n",
        "\r\n",
        "    daily_jobs = [\r\n",
        "        (\"\", \"schools\", \"cursor\", Limit, \"\", False, False), \r\n",
        "        (\"/students\", \"students\", \"cursor\", Limit, \"\", False, False), \r\n",
        "        (\"/students\", \"students_education\", \"cursor\", Limit, \"&include=education_details\", False, False), \r\n",
        "        (\"/students\", \"students_extended\", \"cursor\", Limit, \"&include=extended_details\", False, False), \r\n",
        "        (\"/attendance-summaries\", \"attendance-summaries\", \"cursor\", Limit, \"\", False, False),\r\n",
        "        #(\"/behaviours\", \"behaviours_students\", \"cursor\", Limit, \"&include=students\", False, True),\r\n",
        "        #(\"/achievements\", \"achievements_students\", \"offset\", Limit, \"&include=students\", False, True),\r\n",
        "        ]\r\n",
        "\r\n",
        "    # call load bronze for each of the daily jobs\r\n",
        "    for job in daily_jobs:\r\n",
        "        #load_bronze(spark, job[0], job[1], school[\"school_id\"], school[\"token\"], job[2], job[3], job[4], job[5], job[6])\r\n",
        "        # to override the lastupdated:\r\n",
        "        load_bronze(spark, job[0], job[1], school_id, token, job[2], job[3], job[4], job[5], job[6], override_date=\"2024-01-01 00:00:00\")\r\n",
        "\r\n",
        "\r\n",
        "    # Save the audit log\r\n",
        "    oeai.save_audit_log(spark, audit_log, bronze_path + \"audit_log.json\")\r\n",
        ""
      ]
    }
  ],
  "metadata": {
    "save_output": true,
    "kernelspec": {
      "name": "synapse_pyspark",
      "display_name": "Synapse PySpark"
    },
    "language_info": {
      "name": "python"
    }
  }
}